# ACE-Step 1.5 전체 가이드 (필독)

**언어 / Language / 语言 / 言語:** [English](Tutorial.md) | [한국어](Tutorial.md) | [中文](../zh/Tutorial.md) | [日本語](../ja/Tutorial.md)

---

안녕하세요, ACE-Step 개발자 공준민(Gong Junmin)입니다. 이 튜토리얼을 통해 ACE-Step 1.5의 설계 철학과 사용법을 안내해 드리겠습니다.

## 멘탈 모델 (Mental Models)

시작하기에 앞서, 적절한 기대를 갖기 위해 올바른 멘탈 모델을 세울 필요가 있습니다.

### 인간 중심 디자인 (Human-Centered Design)

이 모델은 **'원클릭 생성'**을 위해 설계된 것이 아니라, **'인간 중심의 생성'**을 위해 설계되었습니다.

이 차이를 이해하는 것이 매우 중요합니다.

### 원클릭 생성(One-Click Generation)이란 무엇인가요?

프롬프트를 입력하고 생성 버튼을 누른 뒤, 몇 가지 버전을 들어보고 마음에 드는 것을 골라 사용하는 방식입니다. 다른 사람이 같은 프롬프트를 입력하면 아마 비슷한 결과를 얻게 될 것입니다.

이 모드에서 여러분과 AI는 **'고객-공급업체'** 관계입니다. 여러분은 명확한 목적과 막연한 기대치를 가지고 오며, AI가 그 기대치에 근접한 제품을 제공하기를 바랍니다. 본질적으로 구글 검색이나 스포티파이에서 노래를 찾는 것과 크게 다르지 않으며, 약간의 커스터마이징이 추가된 정도입니다.

여기서 AI는 서비스일 뿐, 창의적인 영감을 주는 존재는 아닙니다.

Suno, Udio, MiniMax, Mureka 등의 플랫폼은 모두 이러한 철학으로 설계되었습니다. 그들은 서비스를 제공하기 위해 모델의 규모를 키웁니다. 여러분이 생성한 음악은 그들의 약관에 묶여 있습니다. 로컬에서 실행할 수 없고, 개인화된 탐구를 위해 미세 조정(fine-tune)할 수도 없습니다. 그들이 몰래 모델이나 약관을 변경하더라도 여러분은 받아들일 수밖에 없습니다.

### 인간 중심의 생성(Human-Centered Generation)이란 무엇인가요?

AI 레이어를 약화시키고 인간 레이어를 강화하여, 인간의 의지, 창의성, 영감이 AI에 생명력을 불어넣게 하는 것이 인간 중심의 생성입니다.

원클릭 생성의 강한 목적성과 달리, 인간 중심의 생성은 더 **'유희적인(playful)'** 성격을 띱니다. 여러분과 모델이 **'협력자'**가 되어 즐기는 인터랙티브 게임에 가깝습니다.

워크플로우는 이렇습니다. 영감의 씨앗을 던져 몇 곡을 얻고, 그중에서 흥미로운 방향을 선택하여 계속 반복(iteration)합니다.
- 프롬프트를 조정하여 다시 생성
- **Cover** 기능을 사용하여 구조는 유지하면서 세부 사항 조정
- **Repaint** 기능을 사용하여 국소적인 수정
- **Add Layer**를 사용하여 악기 레이어 추가 또는 제거

이 지점에서 AI는 여러분의 하인이 아니라 **'영감 제공자(inspirer)'**가 됩니다.

### 이 디자인이 충족해야 할 조건은 무엇인가요?

인간 중심의 워크플로우가 제대로 작동하려면 모델이 몇 가지 핵심 조건을 충족해야 합니다.

**첫째, 오픈 소스여야 하며 로컬에서 실행 및 학습이 가능해야 합니다.**

이것은 단순히 기술적인 순수주의의 문제가 아니라 소유권의 문제입니다. 폐쇄형 플랫폼을 사용할 때 여러분은 모델을 소유하지 않으며, 생성된 작업물은 그들의 약관에 묶입니다. 버전 업데이트, 약관 변경, 서비스 중단 등 그 무엇도 여러분의 통제 하에 있지 않습니다.

하지만 모델이 오픈 소스이고 로컬에서 실행 가능하다면 모든 것이 바뀝니다. **여러분은 이 모델을 영원히 소유하며, 이를 통해 만든 모든 창작물 또한 영원히 여러분의 소유입니다.** 제3자의 약관 문제나 플랫폼 리스크 없이 여러분만의 창의적인 시스템을 미세 조정하고, 수정하고, 구축할 수 있습니다. 여러분의 작업물은 영원히 여러분의 것입니다. 마치 악기를 사는 것과 같습니다. 언제 어디서나 사용할 수 있고, 원할 때 언제든 조정할 수 있습니다.

**둘째, 빨라야 합니다.**

인간의 시간은 소중하지만, 더 중요한 것은 **느린 생성이 몰입(flow) 상태를 깨뜨린다**는 점입니다.

인간 중심 워크플로우의 핵심은 "시도하고, 듣고, 조정하는" 빠른 사이클입니다. 생성이 매번 몇 분씩 걸린다면 기다리는 동안 영감은 흩어지고, "놀이"의 경험은 "기다림"의 고통으로 변질됩니다.

따라서 우리는 품질을 보장하면서도 원활한 인간-기계 대화 리듬을 지원할 수 있을 만큼 생성 속도를 최적화했습니다.

### 유한 게임(Finite Game) vs 무한 게임(Infinite Game)

원클릭 생성은 **유한 게임**입니다. 목표가 명확하고 결과 지향적이며 결승선에서 끝납니다. 어떤 면에서 그것은 음악 산업을 차갑게 비우고 많은 사람의 일자리를 대체합니다.

인간 중심의 생성은 **무한 게임**입니다. 재미가 과정에 있고, 그 과정은 결코 끝나지 않기 때문입니다.

우리의 비전은 AI 음악 생성을 민주화하는 것입니다. ACE-Step이 여러분의 주머니 속 큰 장난감이 되게 하고, 음악이 **'Play(연주/놀이)'** 그 자체로 돌아가게 하는 것입니다. 단순히 재생 버튼을 클릭하는 것이 아니라 창의적으로 "노는" 것 말이죠.

---

## 코끼리와 기수 비유 (The Elephant Rider Metaphor)

> 추천 읽을거리: [Suno 마스터를 위한 완전 가이드](https://www.notion.so/The-Complete-Guide-to-Mastering-Suno-Advanced-Strategies-for-Professional-Music-Generation-2d6ae744ebdf8024be42f6645f884221) — 이 블로그 튜토리얼은 AI 음악에 대한 기초적인 이해를 세우는 데 도움이 될 것입니다.

AI 음악 생성은 심리학에서 유명한 **'코끼리와 기수'** 비유와 같습니다.

의식은 무의식 위에 올라타 있고, 인간은 코끼리 위에 타 있습니다. 방향을 제시할 수는 있지만 코끼리가 모든 명령을 정확하고 즉각적으로 수행하게 만들 수는 없습니다. 코끼리는 자신만의 관성, 기질, 의지를 가지고 있습니다.

이 코끼리가 바로 음악 생성 모델입니다.

### 빙산 모델 (The Iceberg Model)

오디오와 시맨틱(의미) 사이에는 숨겨진 빙산이 있습니다.

우리가 언어로 묘사할 수 있는 것들 — 스타일, 악기, 음색, 감정, 장면, 전개, 가사, 보컬 스타일 — 은 익숙한 단어들이며 우리가 만질 수 있는 부분입니다. 하지만 이들을 모두 합쳐도 수면 위로 드러난 오디오 빙산의 아주 작은 일부분일 뿐입니다.

가장 정밀한 제어는 무엇일까요? 예상되는 오디오를 입력하면 모델이 그것을 그대로 변함없이 반환하는 것입니다.

하지만 텍스트 설명, 참조, 프롬프트를 사용하는 한 모델은 상상의 나래를 펼칠 공간을 갖게 됩니다. 이것은 버그가 아니라 사물의 본질입니다.

### 코끼리는 무엇인가요?

이 코끼리는 수많은 요소의 융합체입니다. 데이터 분포, 모델 규모, 알고리즘 설계, 어노테이션(주석) 편향, 평가 편향 — **이것은 인류 음악 역사와 엔지니어링적 절충안이 추상적으로 결정화된 것입니다.**

이러한 요소들 중 하나라도 어긋난다면 여러분의 취향과 기대를 정확하게 반영하지 못할 것입니다.

물론 우리는 데이터 규모를 확장하고, 알고리즘 효율을 높이고, 어노테이션의 정밀도를 높이고, 모델 용량을 키우고, 더 전문적인 평가 시스템을 도입할 수 있습니다. 이것들은 모델 개발자로서 우리가 최적화할 수 있는 방향입니다.

하지만 언젠가 기술적 "완벽함"에 도달한다 하더라도 피할 수 없는 근본적인 문제가 있습니다. 바로 **취향(taste)**입니다.

### 취향과 기대치

취향은 사람마다 다릅니다.

음악 생성 모델이 모든 청취자를 만족시키려 한다면, 그 출력물은 인류 음악 역사의 대중적 평균으로 수렴할 것입니다. **이것은 극도로 평범할 것입니다.**

소리에 의미, 감정, 경험, 삶, 문화적 상징 가치를 부여하는 것은 인간입니다. 독특한 취향을 만들어내는 것은 소수의 아티스트들이며, 그들이 일반 대중이 소비하고 따르게 만들며 비주류를 주류로 바꿉니다. 이러한 선구적인 소수 아티스트들이 전설이 됩니다.

따라서 모델의 결과물이 "내 취향이 아니다"라고 느낀다면, 그것은 모델의 문제가 아니라 **여러분의 취향이 그 "평균" 밖에 있기 때문일 수 있습니다.** 이것은 좋은 일입니다.

즉, **여러분은 코끼리가 자동으로 여러분을 이해하기를 기대하기보다 코끼리를 가이드하는 법을 배워야 합니다.**

---

## 코끼리 떼 이해하기: 모델 아키텍처 및 선택

이제 "코끼리" 비유를 이해하셨을 겁니다. 사실 이것은 —

**한 마리의 코끼리가 아니라 크고 작은 코끼리들이 가족을 이룬 코끼리 떼입니다.** 🐘🐘🐘🐘

### 아키텍처 원칙: 두 개의 뇌

ACE-Step 1.5는 두 개의 핵심 구성 요소가 함께 작동하는 **하이브리드 아키텍처**를 사용합니다.

```
사용자 입력 → [5Hz LM] → 시맨틱 청사진 → [DiT] → 오디오
               ↓
         메타데이터 추론
         캡션 최적화
         구조 계획
```

**5Hz LM (언어 모델) — 기획자 (선택 사항)**

LM은 여러분의 의도를 이해하고 계획을 세우는 담당자입니다.
- **Chain-of-Thought**를 통해 음악 메타데이터(BPM, 키, 길이 등)를 추론합니다.
- 여러분의 캡션을 최적화하고 확장하여 의도를 이해하고 보충합니다.
- **시맨틱 코드(semantic codes)**를 생성합니다. 여기에는 작곡 멜로디, 편곡, 일부 음색 정보가 암시적으로 포함되어 있습니다.

LM은 학습 데이터로부터 **세상에 대한 지식**을 배웁니다. 사용성을 개선하고 프로토타입을 빠르게 생성하도록 돕는 기획자입니다.

**하지만 LM이 반드시 필요한 것은 아닙니다.**

원하는 것이 매우 명확하거나 계획이 이미 서 있다면 `thinking` 모드를 사용하지 않음으로써 LM 계획 단계를 완전히 건너뛸 수 있습니다.

예를 들어, **Cover 모드**에선 참조 오디오를 사용하여 작곡, 화음, 구조를 제약하고 DiT가 직접 생성하게 합니다. 여기서 여러분은 LM의 작업을 대신하는 **기획자 본인**이 됩니다.

또 다른 예로 **Repaint 모드**에선 참조 오디오를 컨텍스트로 사용하여 음색, 믹싱, 세부 사항을 제약하고 DiT가 국소적으로 직접 조정하게 합니다. 여기서 DiT는 창의적인 아이디어를 돕고 불협화음을 고치는 브레인스토밍 파트너에 가깝습니다.

**DiT (Diffusion Transformer) — 실행자**

DiT는 계획을 현실로 바꾸는 "오디오 장인"입니다.
- LM이 생성한 시맨틱 코드와 조건을 전달받습니다.
- **확산 공정(diffusion process)**을 통해 노이즈로부터 오디오를 점진적으로 "조각"해냅니다.
- 최종적인 음색, 믹싱, 세부 사항을 결정합니다.

**왜 이렇게 설계했나요?**

전통적인 방식은 디퓨전 모델이 텍스트로부터 오디오를 직접 생성하게 하지만, 텍스트-오디오 매핑은 너무 모호합니다. ACE-Step은 중간 레이어로 LM을 도입했습니다.
- LM은 시맨틱 이해와 계획에 능숙합니다.
- DiT는 고음질 오디오 생성에 능숙합니다.
- 두 모델이 각자의 역할에 집중하며 협력합니다.

### 기획자 선택: LM 모델

LM에는 네 가지 옵션이 있습니다: **LM 미사용** (thinking 모드 비활성), **0.6B**, **1.7B**, **4B**.

학습 데이터는 완전히 동일하며, 차이는 순수하게 **지식 용량**에 있습니다.
- 모델이 클수록 세상에 대한 지식이 풍부합니다.
- 모델이 클수록 기억력이 좋습니다 (예: 참조 오디오 멜로디 기억).
- 모델이 클수록 희귀한 스타일이나 악기에 대해 상대적으로 더 잘 작동합니다.

| 선택 | 속도 | 지식 수준 | 기억력 | 활용 사례 |
|--------|:-----:|:---------------:|:------:|-----------|
| No LM | ⚡⚡⚡⚡ | — | — | 직접 기획할 때 (예: Cover 모드) |
| `0.6B` | ⚡⚡⚡ | 기초 | 낮음 | 저사양 GPU (< 8GB), 빠른 프로토타이핑 |
| `1.7B` | ⚡⚡ | 중간 | 중간 | **기본 권장 사항** |
| `4B` | ⚡ | 풍부 | 강함 | 복잡한 작업, 고품질 생성 |

**어떻게 선택하나요?**

하드웨어 사양에 맞춰 선택하세요.
- **VRAM < 8GB** → LM 미사용 또는 `0.6B`
- **VRAM 8–16GB** → `1.7B` (기본값)
- **VRAM > 16GB** → `1.7B` 또는 `4B`

### 실행자 선택: DiT 모델

계획이 세워졌다면 실행자를 선택해야 합니다. DiT는 ACE-Step 1.5의 핵심으로, 다양한 작업을 처리하고 LM이 생성한 코드를 해석하는 방식을 결정합니다.

우리는 **4개의 Turbo 모델**, **1개의 SFT 모델**, **1개의 Base 모델**을 제공합니다.

#### Turbo 시리즈 (일상적 사용 권장)

Turbo 모델은 증류(distillation) 학습을 거쳐 단 8단계 만에 고품질 오디오를 생성합니다. 네 가지 변형 모델의 핵심적인 차이는 증류 과정에서의 **시프트(shift) 하이퍼파라미터 설정**입니다.

**시프트(shift)란 무엇인가요?**

시프트는 DiT 디노이징 과정에서 "주의 집중 배분"을 결정합니다.
- **큰 시프트** → 초기 디노이징에 더 많은 노력을 들여 **강한 시맨틱**과 명확한 골격을 구축합니다.
- **작은 시프트** → 균등한 단계 배분으로 **더 많은 세부 사항**을 표현하지만, 노이즈가 섞일 수 있습니다.

쉽게 이해하자면: 높은 시프트는 "외곽선을 먼저 그리고 세부 사항을 채우는" 방식이고, 낮은 시프트는 "그리면서 동시에 수정해 나가는" 방식입니다.

| 모델 | 증류 설정 | 특징 |
|-------|---------------------|-----------------|
| `turbo` (기본) | shift 1, 2, 3 공동 증류 | **창의성과 시맨틱의 최적 균형**, 가장 추천하는 모델 |
| `turbo-shift1` | shift=1 전용 증류 | 세부 표현이 풍부하지만 시맨틱은 상대적으로 약함 |
| `turbo-shift3` | shift=3 전용 증류 | 더 명확하고 풍부한 음색, 하지만 건조하게 들릴 수 있음 |
| `turbo-continuous` | 실험적 변형 | 1~5 사이의 연속적인 시프트 조절 지원 |

음악 스타일에 따라 선호하는 모델을 선택할 수 있습니다. **처음에는 가장 균형 잡힌 `turbo` 모델로 시작하는 것을 권장합니다.**

#### SFT 모델

Turbo와 비교하여 SFT 모델은 두 가지 특징이 있습니다.
- **CFG (Classifier-Free Guidance) 지원**: 프롬프트 준수 능력을 미세 조정할 수 있습니다.
- **더 많은 단계 (50 steps)**: 모델이 더 오래 "생각"할 시간을 줍니다.

단점은 단계가 많아질수록 에러가 누적되어 오디오의 선명도가 Turbo보다 약간 떨어질 수 있다는 점입니다. 하지만 **세부 묘사와 시맨틱 파싱 능력은 더 뛰어납니다.**

#### Base 모델

Base 모델은 **모든 작업의 마스터**이며, SFT나 Turbo에는 없는 3가지 전용 작업을 지원합니다.

| 작업 | 설명 |
|------|-------------|
| `extract` | 믹스된 오디오에서 단일 트랙 추출 (예: 보컬 제거) |
| `lego` | 기존 트랙에 새로운 트랙 추가 (예: 기타에 드럼 추가) |
| `complete` | 단일 트랙에 믹스된 반주 추가 (예: 보컬에 기타+드럼 반주 추가) |

또한 Base 모델은 **가소성이 가장 높습니다.** 대규모 미세 조정이 필요한 경우 Base 모델로 실험을 시작하는 것이 좋습니다.

---

## 코끼리 가이드하기: 무엇을 제어할 수 있나요?

이제 코끼리 떼에 대해 알게 되었으니, 그들과 소통하는 법을 배워봅시다.

모든 생성은 **입력 제어**, **추론 하이퍼파라미터**, **랜덤 요소**라는 세 가지 요인에 의해 결정됩니다.

### I. 입력 제어: 무엇을 원하는가?

이 부분은 모델과 "창작 의도"를 소통하는 부분입니다.

| 카테고리 | 파라미터 | 역할 |
|----------|-----------|----------|
| **작업 유형** | `task_type` | 생성 모드 결정: text2music, cover, repaint, lego, extract, complete |
| **텍스트 입력** | `caption` | 음악 전반에 대한 설명: 스타일, 악기, 감정, 분위기, 음색, 보컬 성별, 전개 등 |
| | `lyrics` | 시간에 따른 전개 설명: 가사 내용, 음악 구조 진화, 보컬 변화, 연주 스타일 등 (연주곡은 `[Instrumental]` 사용) |
| **음악 메타데이터** | `bpm` | 템포 (30–300) |
| | `keyscale` | 키 (예: C Major, Am) |
| | `timesignature` | 박자 기호 (4/4, 3/4, 6/8) |
| | `vocal_language` | 보컬 언어 |
| | `duration` | 목표 길이 (초) |
| **오디오 참조** | `reference_audio` | 음색이나 스타일에 대한 전역 참조 (속성 전이용) |
| | `src_audio` | 오디오 기반 작업의 소스 파일 |
| | `audio_codes` | 시맨틱 코드 직접 입력 (고급 사용자용) |
| **구간 제어** | `repainting_start/end` | 작업 시간 범위 (repaint 구역 등) |

---

#### Caption에 대하여: 가장 중요한 입력

**Caption(캡션)은 생성된 음악에 영향을 주는 가장 중요한 요소입니다.**

단순한 스타일 단어, 쉼표로 구분된 태그, 복잡한 자연어 설명 등 다양한 형식을 지원합니다.

**좋은 캡션을 쓰기 위한 5가지 방법:**

1. **무작위 주사위** — UI의 랜덤 버튼을 클릭하여 예제 캡션이 어떻게 작성되었는지 확인하세요. 이를 템플릿으로 사용할 수 있습니다.
2. **Format 자동 재작성** — 간단히 쓴 내용을 LLM이 자동으로 확장해 주는 `format` 기능을 활용하세요.
3. **CoT 재작성** — LM이 초기화된 경우, Chain-of-Thought를 통해 캡션을 최적화하고 확장할 수 있습니다.
4. **오디오에서 캡션 추출** — 입력 오디오를 캡션으로 변환하는 기능도 제공합니다. 완벽하진 않아도 시작점으로 충분합니다.
5. **Simple Mode** — 간단한 곡 설명만 입력하면 LM이 캡션, 가사, 메타데이터를 모두 자동으로 생성합니다.

**캡션 작성을 위한 차원들:**
주장(Style), 감정(Emotion), 악기(Instruments), 음색 질감(Timbre Texture), 시대(Era), 프로덕션 스타일(Production Style), 보컬 특징(Vocal), 속도/리듬(Speed) 등을 조합하세요.

**실질적인 원칙:**
1. **구체적으로 쓰세요** — "슬픈 노래"보다는 "피아노 발라드와 허스키한 여성 보컬"이 낫습니다.
2. **여러 차원을 조합하세요** — 스타일+감정+악기+질감을 조합하면 의도를 더 정확히 고착시킬 수 있습니다.
3. **충돌하는 단어를 피하세요** — "클래식 현악기"와 "하드코어 메탈"을 동시에 원하면 결과가 모호해질 수 있습니다.

---

#### 가사(Lyrics)에 대하여: 시간적 시나리오

캡션이 음악의 "전체 초상화"라면, **가사는 음악의 "시간적 시나리오"**입니다.
- 가사 텍스트 그 자체
- **구조 태그** ([Verse], [Chorus], [Bridge]...)
- **보컬 스타일 힌트** ([raspy vocal], [whispered]...)
- **악기 연주 구간** ([guitar solo], [drum break]...)
- **에너지 변화** ([building energy], [explosive drop]...)

**구조 태그가 핵심입니다.**
각 섹션이 시작될 때 `[Verse]`, `[Chorus]` 등의 태그를 사용하여 모델에게 현재 구간의 성격을 알려주세요. `[Chorus - anthemic]`과 같이 세부 묘사를 추가할 수도 있습니다.

**모델과의 일관성 유지:**
캡션에서 "피아노 발라드"라고 했다면 가사 구간에서 `[Guitar Solo - electric]`과 같은 충돌하는 태그를 쓰지 않도록 주의하세요.

---

#### 음악 메타데이터: 선택적인 미세 제어

대부분의 경우 메타데이터를 직접 설정할 필요는 없습니다. `thinking` 모드를 켜면 LM이 캡션과 가사를 바탕으로 적절한 BPM, 키, 박자를 스스로 결정합니다.

하지만 명확한 요구 사항이 있다면 직접 `bpm`, `keyscale`, `timesignature` 등을 설정할 수 있습니다. 이는 "엄밀한 명령"이라기보다 모델이 참고하는 **"가이드라인"**임을 명심하세요.

---

#### 오디오 제어: 소리로 소리 제어하기

**텍스트는 추상화된 전달이지만, 가장 강력한 제어는 여전히 오디오를 통하는 방식입니다.**

1. **참조 오디오 (Reference Audio)**: 음색, 믹싱 스타일, 연주 기법 등 **전반적인 질감**을 제어합니다.
2. **소스 오디오 (Source Audio - Cover)**: **멜로디 구조**를 제어합니다. 원곡의 구조를 유지하면서 스타일만 바꾸고 싶을 때 사용합니다.
3. **컨텍스트 기반 제어 (Repaint)**: 기존 오디오의 **특정 구간을 수정하거나 이어 쓰기** 위해 사용합니다. 인트로, 중간 구간, 엔딩 등 어디든 가능합니다.

---

## 결론

이 튜토리얼은 ACE-Step 1.5의 핵심 개념과 사용법을 다루었습니다.
- **멘탈 모델**: 인간 중심의 생성 철학 이해
- **아키텍처**: LM과 DiT의 협업 이해
- **입력 제어**: 텍스트 및 오디오를 통한 제어 방법 마스터
- **추론 하이퍼파라미터**: 생성 공정에 영향을 주는 파라미터 이해
- **랜덤 요소**: 랜덤성을 탐색 도구로 활용하고 자동화 툴로 효율을 높이는 법

이것은 시작일 뿐입니다. 더 많은 프롬프트 팁, 작업별 상세 가이드, 고급 기술 등을 계속 업데이트할 예정입니다. 질문이나 제안이 있다면 언제든 피드백을 주세요. ACE-Step이 여러분의 훌륭한 창의적 파트너가 되기를 바랍니다.

---

*계속될 예정입니다...*
